{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9ef097-fadc-4399-bcd5-3121d0d6c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62fa22b8-ff09-4805-81d9-9c96d2aff95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0+cu124'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3c41d9b-c4e5-47d7-a4b6-68929baca340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae905d7f-aa88-46c9-923b-fdf37fcab148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/unsw/17D1-7AB7/mostafa/audioLLM/Qwen'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8615819f-b11f-4672-9523-a1b5544fc287",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  5 10:14:35 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:04:00.0 Off |                  Off |\n",
      "| 30%   45C    P8              15W / 300W |   7356MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro K620                    On  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 34%   38C    P8               1W /  30W |     61MiB /  2048MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1880      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    174640      C   python                                     4392MiB |\n",
      "|    0   N/A  N/A   2484308      C   ...ostafa/PhoneAid/venv/bin/python3.12     2946MiB |\n",
      "|    1   N/A  N/A      1880      G   /usr/lib/xorg/Xorg                           51MiB |\n",
      "|    1   N/A  N/A      2008      G   /usr/bin/gnome-shell                          4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c69b35-6251-4d9c-9101-6011dc6a5372",
   "metadata": {},
   "source": [
    "# Trelis Fine-tune Bird dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140515e-652b-4a08-a6a4-2a7c7d3ad4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d758c-4a31-403e-a849-10c5bd8ee3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18db7ee0-9620-48e9-92df-a6a469448955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in /home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages (from tensorboardX) (2.1.3)\n",
      "Requirement already satisfied: packaging in /home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages (from tensorboardX) (24.2)\n",
      "Collecting protobuf>=3.20 (from tensorboardX)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-5.29.3 tensorboardX-2.6.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfebe28-4298-4409-bb5a-5bbf001488f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fast download\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f65a70-d8e4-43be-88e3-8c48bed687e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "from PIL import Image\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "torch_dtype = torch.bfloat16 #Brain Floating Point: bfloat16 is a 16-bit floating-point format designed for machine learning. It's like a shorter version of the standard 32-bit floating-point (float32) format.\n",
    "\n",
    "model_id = 'Qwen/Qwen2-Audio-7B-Instruct'\n",
    "\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch_dtype,\n",
    "    trust_remote_code = True\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db042bcd-2bd5-431e-ac98-dac269029e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54f3b6-020e-4dd4-acbe-c39328de6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PROMPT = \"Identify the bird species in the audio clip\"\n",
    "debug = False\n",
    "\n",
    "def run_example(audio_url):\n",
    "    try:\n",
    "        #load audio\n",
    "        audio_data, sample_rate = librosa.load(BytesIO(urlopen(audio_url).read()), sr=None)\n",
    "        #play audio\n",
    "        display(Audio(audio_data, rate=sample_rate))\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"audio\", \"audio_url\": audio_url},\n",
    "                {\"type\": \"text\", \"text\": TASK_PROMPT},\n",
    "            ]}\n",
    "        ]\n",
    "        # process conversation\n",
    "        text = processor.apply_chat_template(conversation, add_generation_prompt= True, tokenize=False)\n",
    "        if debug:\n",
    "            print(f\"Templated text:\\n{text}\")\n",
    "            \n",
    "        audios = []\n",
    "    \n",
    "        for message in conversation:\n",
    "            if isinstance(message[\"content\"], list):\n",
    "                for item in message[\"content\"]:\n",
    "                    if isinstance(item, dict) and item[\"type\"] == \"audio\" and \"audio_url\" in item:\n",
    "                        try:\n",
    "                            sr = int(processor.feature_extractor.sampling_rate)\n",
    "                            aduio_data,_ = librosa.load(BytesIO(urlopen(item[\"audio_url\"]).read()), sr=sr)\n",
    "                            audios.append(aduio_data)\n",
    "                        except Exception as e:\n",
    "                            print(f\"failed to process audio from {item['audio_url']}, {e}\")\n",
    "        inputs = processor(\n",
    "            text=text,\n",
    "            audios=audios,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        generate_ids = model.generate(**inputs, max_length=1024)\n",
    "\n",
    "        print(f\"Input shape: {inputs.input_ids.size(1)}, Generated shape: {generate_ids.shape}\")\n",
    "        generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "        \n",
    "        response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "       \n",
    "audio_url = \"https://xeno-canto.org/841748/download\"\n",
    "run_example(audio_url)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dbc6b1-8c62-49c2-af30-33459a82175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Trelis/bird-songs\")\n",
    "\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b9b9df4-3d16-46aa-9464-73ba4ec2cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image, resize\n",
    "\n",
    "def run_model_evaluation(dataset):\n",
    "    for example in dataset:\n",
    "        audio = example[\"url\"]\n",
    "        results = run_example(audio)\n",
    "        print(f\"Ground truth answer: {example['name']}\")\n",
    "        print(\"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef37db-c4a5-48a5-b3b1-0c60cb9f4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_evaluation(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92207dea-a09c-4425-85d3-7a7fd5153a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "___\n",
      "<|im_end|>\n",
      "151645\n",
      "___\n",
      "<|endoftext|>\n",
      "151643\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.bos_token)\n",
    "print(processor.tokenizer.bos_token_id)\n",
    "print(\"___\")\n",
    "\n",
    "print(processor.tokenizer.eos_token)\n",
    "print(processor.tokenizer.eos_token_id)\n",
    "print(\"___\")\n",
    "\n",
    "print(processor.tokenizer.pad_token)\n",
    "print(processor.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0139af32-9f80-4348-9554-19730e5f4a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480000\n"
     ]
    }
   ],
   "source": [
    "print(processor.feature_extractor.n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c052f709-0225-47ae-98d5-858a965185c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62493f7f-1d81-44f6-b069-881f4047af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List\n",
    "from torch.utils.data import DataLoader\n",
    "debug = False\n",
    "class AudioDataCollator:\n",
    "    def __init__(self, processor, task_prompt: str, max_length: int = None):\n",
    "        self.processor = processor\n",
    "        self.task_prompt = task_prompt\n",
    "        self.max_length = max_length or processor.feature_extractor.n_samples\n",
    "        self.sampling_rate = processor.feature_extractor.sampling_rate\n",
    "\n",
    "    def process_audio(self, audio_url: str) -> np.ndarray:\n",
    "        try:\n",
    "            audio, sr = librosa.load(BytesIO(urlopen(audio_url).read()), sr=self.sampling_rate)\n",
    "            \n",
    "            if len(audio) > self.max_length:\n",
    "                if debug:\n",
    "                    print(f\"Trimming audio to {self.max_length} samples\")\n",
    "                audio = audio[:self.max_length]\n",
    "            return audio.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing audio: {e}\")\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str,Any]]) -> Dict[str, torch.Tensor]:\n",
    "        valid_examples = []\n",
    "        audios = []\n",
    "        combined_texts = []\n",
    "        for example in examples:\n",
    "            try:\n",
    "                audio = self.process_audio(example[\"url\"])\n",
    "                audios.append(audio)\n",
    "\n",
    "                conversation = [\n",
    "                    {\"role\": \"user\", \"content\":[\n",
    "                        {\"type\": \"audio\", \"audio_url\": example[\"url\"]},\n",
    "                        {\"type\": \"text\", \"text\": self.task_prompt}\n",
    "                    ]},\n",
    "                    {\"role\": \"assistant\", \"content\": example[\"name\"]}\n",
    "                ]\n",
    "                combined_text = self.processor.apply_chat_template(\n",
    "                    conversation, add_generation_prompt=False, tokenize=False  #Here add_generation_prompt=False as it's training and the answer is in the conversation, and tokenize=false as we will do tokenize later\n",
    "                )\n",
    "\n",
    "                combined_texts.append(combined_text)\n",
    "                valid_examples.append(example)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {example['id']}: {e}\")\n",
    "                continue\n",
    "        if not valid_examples:\n",
    "            raise ValueError(\"No valid examples found\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Number of valid examples: {len(valid_examples)}\")\n",
    "            print(f\"Number of audios: {len(audios)}\")\n",
    "            print(f\"Number of combined texts: {len(combined_texts)}\")\n",
    "\n",
    "        try:\n",
    "            inputs = self.processor(\n",
    "                text=combined_texts,\n",
    "                audios=audios,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing inputs: {e}\")\n",
    "\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Input ids shape: {inputs['input_ids'].shape}\")\n",
    "            print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "        # Mask the prompt portion\n",
    "        for i in range(len(combined_texts)):\n",
    "            try:\n",
    "                input_ids_row = inputs[\"input_ids\"][i]\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing example {i}\")\n",
    "                    print(f\"Input IDs for this example: {input_ids_row}\")\n",
    "\n",
    "                assistant_start_tokens = self.processor.tokenizer.encode(\"<|im_start|>assistant\")\n",
    "                if debug:\n",
    "                    print(f\"Assistant start tokens: {assistant_start_tokens}\")\n",
    "\n",
    "                assistant_start_idx = -1\n",
    "                for j in range(len(input_ids_row) - len(assistant_start_tokens) + 1):\n",
    "                    if torch.equal(input_ids_row[j:j+len(assistant_start_tokens)], torch.tensor(assistant_start_tokens)):\n",
    "                        assistant_start_idx = j\n",
    "                        break\n",
    "                if debug:\n",
    "                    print(f\"Assistant start index: {assistant_start_idx}\")\n",
    "\n",
    "                if assistant_start_idx != -1:\n",
    "                    labels[i, :assistant_start_idx + len(assistant_start_tokens)] = -100\n",
    "                else:\n",
    "                    print(f\"Error: Assistant start index not found for example {i}\")\n",
    "                    labels[i,:] = -100\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {i}: {e}\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Input ids shape after masking: {inputs['input_ids'].shape}\")\n",
    "            print(f\"Attention mask shape after masking: {inputs['attention_mask'].shape}\")\n",
    "            print(f\"Labels shape after masking: {labels.shape}\")\n",
    "            print(f\"First label row after masking: {labels[0]}\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"input_features\": inputs[\"input_features\"],\n",
    "            \"feature_attention_mask\": inputs[\"feature_attention_mask\"],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "                \n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a92da80e-fdf3-4c62-8d79-78ff6eb2f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(batch: Dict[str, torch.Tensor], processor) -> None:\n",
    "    print(\"\\n===== INSPECTING BATCH ====\")\n",
    "    \n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"\\n{key}\")\n",
    "        print(f\" shape: {tensor.shape}\")\n",
    "        print(f\" dtype: {tensor.dtype}\")\n",
    "        print(f\" device: {tensor.device}\")\n",
    "        if key not in ['input_features', 'feature_attention_mask']:\n",
    "            print(f\" First element: {tensor[0].tolist()}\")\n",
    "\n",
    "    if \"input_ids\" in batch:\n",
    "        print(\"\\n=== Decoded input_ids ===\")\n",
    "        first_input = batch[\"input_ids\"][0].tolist()\n",
    "        decoded_input = processor.tokenizer.decode(first_input, skip_special_tokens=False)\n",
    "        print(f\"Decoded Input (First Row):\\n{decoded_input}\")\n",
    "\n",
    "    if \"labels\" in batch:\n",
    "        print(\"\\n=== Decoded labels ===\")\n",
    "        first_label = batch[\"labels\"][0].tolist()\n",
    "        valid_tokens = [token for token in first_label if token != -100]\n",
    "        decoded_label = processor.tokenizer.decode(valid_tokens, skip_special_tokens=False)\n",
    "        print(f\"Decoded Label (First Row):\\n{decoded_label}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77131658-2048-4dca-a3ad-77491d3976f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = AudioDataCollator(\n",
    "    processor=processor,\n",
    "   task_prompt = \"Identify the bird species in this audio clip:\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d2d9e3-4bf6-4f36-a0b9-4b20e29949a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ae186-2c70-431c-bdd8-142b501f4dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33958f06-0d7d-4ec6-b235-de18a68c8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4d31f-c902-4689-9788-cfa8fbfa2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_batch(batch, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ba703e3-e045-4cf6-9816-aebc7815ecd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.encode(\"<|im_start|>assistant\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f757b16-42ee-40ec-99a4-c68c514f42cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2AudioForConditionalGeneration(\n",
      "  (audio_tower): Qwen2AudioEncoder(\n",
      "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (embed_positions): Embedding(1500, 1280)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Qwen2AudioEncoderLayer(\n",
      "        (self_attn): Qwen2AudioSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (avg_pooler): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (multi_modal_projector): Qwen2AudioMultiModalProjector(\n",
      "    (linear): Linear(in_features=1280, out_features=4096, bias=True)\n",
      "  )\n",
      "  (language_model): Qwen2ForCausalLM(\n",
      "    (model): Qwen2Model(\n",
      "      (embed_tokens): Embedding(156032, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2Attention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): Qwen2RotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=156032, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e75d99df-d215-49fb-868c-c136f16f0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed117400-fac9-4b21-bdb7-40469711bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    r=32,\n",
    "    #target_module_size=64,\n",
    "    lora_alpha=32,\n",
    "    use_rslora=True,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe66118-86af-43fc-bb47-4852f9e73f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=32, target_modules='all-linear', exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "191ab61a-e5f8-40aa-90eb-b0927cf8e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108,843,008 || all params: 8,505,937,920 || trainable%: 1.2796\n"
     ]
    }
   ],
   "source": [
    "model = peft.get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2dfa7c6-f160-46e4-9dd0-78ab91e97f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "# epochs=1\n",
    "# scheduler = \"constant\"\n",
    "\n",
    "#optional\n",
    "\n",
    "epochs =1\n",
    "scheduler = \"cosine\"\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "run_name = f\"run_lr_{lr}_epochs_{epochs}_scheduler_{scheduler}_batch_size_{batch_size}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    output_dir=f\"./results/{run_name}\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    lr_scheduler_type=scheduler,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    run_name=run_name,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f0cbe6-5a31-4d7e-8a86-4b1191c61dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 07:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.537067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.763963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.741654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>0.655982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.632780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=0.38316794894635675, metrics={'train_runtime': 475.7483, 'train_samples_per_second': 0.168, 'train_steps_per_second': 0.042, 'total_flos': 2995776001646592.0, 'train_loss': 0.38316794894635675, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe576e0-b4ef-42fe-b070-f4b01d7b3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a37465-16dc-4236-ae0b-e20963648572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "run_model_evaluation(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c737f-5d42-4e15-8baf-0fff8d6d88c0",
   "metadata": {},
   "source": [
    "# CI TestBench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9bd40-95ea-40e8-9d52-c281c0b28b19",
   "metadata": {},
   "source": [
    "## Create Dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3420cd50-5d5e-42ee-94e0-3b2be8e1ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset should include these columns\n",
    "# - Audio\n",
    "# - Prompt\n",
    "# - Answer\n",
    "# - text\n",
    "# - language\n",
    "# - age\n",
    "# - gender\n",
    "# - Type of assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76a64b3-9e6c-495e-9748-d975f716e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAUKADIAL data\n",
    "# Create single GT file\n",
    "test_gt = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test_groudtruth.csv'\n",
    "test_md = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test_meta.csv'\n",
    "\n",
    "test_gt_df = pd.read_csv(test_gt)\n",
    "test_md_df = pd.read_csv(test_md)\n",
    "\n",
    "test_df = pd.merge(test_gt_df, test_md_df, how='left', on='tkdname')\n",
    "test_df.to_csv('/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test_groundtruth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2682a30-7e58-499a-bb53-08cdb5015277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tkdname  mmse   dx  age sex\n",
      "0    taukdial-001-1.wav    27   NC   70   F\n",
      "1    taukdial-001-2.wav    27   NC   70   F\n",
      "2    taukdial-001-3.wav    27   NC   70   F\n",
      "3    taukdial-010-1.wav    27  MCI   91   F\n",
      "4    taukdial-010-2.wav    27  MCI   91   F\n",
      "..                  ...   ...  ...  ...  ..\n",
      "115  taukdial-166-2.wav    28  MCI   88   M\n",
      "116  taukdial-166-3.wav    28  MCI   88   M\n",
      "117  taukdial-168-1.wav    29   NC   65   M\n",
      "118  taukdial-168-2.wav    29   NC   65   M\n",
      "119  taukdial-168-3.wav    29   NC   65   M\n",
      "\n",
      "[120 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c70cd2e-2c02-4d25-bf9a-eeab7ae4d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAUKADIAL data\n",
    "import pandas as pd\n",
    "train_gt = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/train_groundtruth.csv'\n",
    "train_path = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/train'\n",
    "\n",
    "test_gt = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test_groundtruth.csv'\n",
    "test_path = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36cd8060-c4cd-41f9-a123-9b6a2a623243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def create_audio_dataset(csv_file, audio_dir, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Creates a Hugging Face Dataset from a CSV file and audio files,\n",
    "    converting to mono and resampling to the target sampling rate using Librosa.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "        audio_dir (str): Path to the directory containing audio files.\n",
    "        target_sr (int): Target sampling rate for resampling (default: 16000).\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: Hugging Face Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    data = {\n",
    "        \"audio\": [],\n",
    "        \"age\": [],\n",
    "        \"sex\": [],\n",
    "        \"mmse\": [],\n",
    "        \"dx\": [],\n",
    "        \"tkdname\": []\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        audio_path = f\"{audio_dir}/{row['tkdname']}\"\n",
    "        try:\n",
    "            # Load audio using librosa, converting to mono and resampling\n",
    "            audio_array, _ = librosa.load(audio_path, sr=target_sr, mono=True)\n",
    "\n",
    "            data[\"audio\"].append({\"path\": audio_path, \"array\": audio_array, \"sampling_rate\": target_sr})\n",
    "            data[\"age\"].append(row[\"age\"])\n",
    "            data[\"sex\"].append(row[\"sex\"])\n",
    "            data[\"mmse\"].append(row[\"mmse\"])\n",
    "            data[\"dx\"].append(row[\"dx\"])\n",
    "            data[\"tkdname\"].append(row['tkdname'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "        except Exception as e: # Catch other librosa related errors.\n",
    "            print(f\"Warning: Error processing {audio_path}: {e}\")\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=target_sr)) #cast the audio column to audio type.\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c377e2e-95c3-460a-a2ab-8c1cddb9cf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'age', 'sex', 'mmse', 'dx', 'tkdname'],\n",
      "    num_rows: 387\n",
      "})\n",
      "{'audio': {'path': None, 'array': array([0.00143433, 0.00158691, 0.00149536, ..., 0.0206604 , 0.02038574,\n",
      "       0.01937866]), 'sampling_rate': 16000}, 'age': 72, 'sex': 'F', 'mmse': 29, 'dx': 'NC', 'tkdname': 'taukdial-002-1.wav'}\n"
     ]
    }
   ],
   "source": [
    "# create train dataset\n",
    "\n",
    "train_audio_dataset = create_audio_dataset(train_gt, train_path, target_sr=16_000)\n",
    "\n",
    "print(train_audio_dataset)\n",
    "print(train_audio_dataset[0]) #prints the first element of the dataset.\n",
    "\n",
    "#optional save to disk.\n",
    "#audio_dataset.save_to_disk(\"your_output_dataset_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2c03a36-34d8-4029-ac98-75efd3ae899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'age', 'sex', 'mmse', 'dx', 'tkdname'],\n",
      "    num_rows: 120\n",
      "})\n",
      "{'audio': {'path': None, 'array': array([-0.00991821, -0.01715088, -0.01513672, ..., -0.00558472,\n",
      "       -0.00704956,  0.        ]), 'sampling_rate': 16000}, 'age': 70, 'sex': 'F', 'mmse': 27, 'dx': 'NC', 'tkdname': 'taukdial-001-1.wav'}\n"
     ]
    }
   ],
   "source": [
    "# create test dataset\n",
    "\n",
    "test_audio_dataset = create_audio_dataset(test_gt, test_path, target_sr=16_000)\n",
    "\n",
    "print(test_audio_dataset)\n",
    "print(test_audio_dataset[0]) #prints the first element of the dataset.\n",
    "\n",
    "#optional save to disk.\n",
    "#audio_dataset.save_to_disk(\"your_output_dataset_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62bd756d-eb4c-4a8b-ae43-3f5c06288aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "taukadial_ds = DatasetDict({'train':train_audio_dataset, 'test':test_audio_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b3050-8fef-461d-98bb-df831701a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Audio(data=train_audio_dataset[0]['audio']['array'], rate=16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fd7cf30-39c5-463a-a2ee-937eb28d2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:02<00:00, 170.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:02<00:00, 54.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "taukadial_ds.save_to_disk(\"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9bace-3a25-4242-8acb-039a9edf42c7",
   "metadata": {},
   "source": [
    "# Create dataset from chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2dbd233-bfa7-489c-837d-3431fd8388fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/train'\n",
    "train_chuncks_cvs = \"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/train_chunks.csv\"\n",
    "test_path = '/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test'\n",
    "test_chuncks_cvs = \"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL-24/test_chunks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de6cc7aa-44a6-4692-9813-facf0acac65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, Audio\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_and_trim_audio(file_path, start_sample, end_sample, target_sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Loads an audio file, converts it to mono, resamples to 16kHz, and trims to the specified sample range.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        start_sample (int): Start sample index.\n",
    "        end_sample (int): End sample index.\n",
    "        target_sample_rate (int, optional): Desired sampling rate (default: 16kHz).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'array' (trimmed waveform) and 'sampling_rate' (16kHz).\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # Convert to mono by averaging channels if it's stereo\n",
    "    if waveform.shape[0] > 1:  # More than 1 channel means stereo\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "\n",
    "    # Resample if needed\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Ensure start and end samples are within bounds\n",
    "    total_samples = waveform.shape[1]\n",
    "    start_sample = min(start_sample, total_samples - 1)\n",
    "    end_sample = min(end_sample, total_samples)\n",
    "\n",
    "    # Trim the waveform\n",
    "    trimmed_waveform = waveform[:, start_sample:end_sample]\n",
    "\n",
    "    return {\"array\": trimmed_waveform.numpy().squeeze(), \"sampling_rate\": target_sample_rate}\n",
    "\n",
    "def create_hf_audio_dataset(csv_path, audio_base_path=None):\n",
    "    \"\"\"\n",
    "    Creates a Hugging Face dataset from a CSV file containing segmented audio metadata.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        audio_base_path (str, optional): Base directory for audio files (if paths need to be adjusted).\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A Hugging Face Dataset with processed audio segments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load CSV into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Adjust file paths if needed\n",
    "    if audio_base_path:\n",
    "        df[\"file_path\"] = df[\"file_path\"].apply(lambda x: os.path.join(audio_base_path, os.path.basename(x)))\n",
    "\n",
    "    # Process audio: convert to mono, resample, and trim\n",
    "    df[\"audio\"] = df.apply(\n",
    "        lambda row: load_and_trim_audio(row[\"file_path\"], row[\"start_sample\"], row[\"end_sample\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Rename text column for consistency\n",
    "    df = df.rename(columns={\"text\": \"transcription\"})\n",
    "\n",
    "    # Convert DataFrame to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85f7769e-77d4-4cc6-bf77-d4854999eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = create_hf_audio_dataset(train_chuncks_cvs,train_path)\n",
    "test_dataset = create_hf_audio_dataset(test_chuncks_cvs,test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e2971e1-4663-43d1-aef0-ec16a9e22d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = DatasetDict({'train':train_dataset, 'test':test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef91a7-696d-4cf1-8787-bcf60a67355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(data=audio_dataset['test'][0]['audio']['array'], rate=16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5c7b270-5e46-4bfc-a911-4b12d25e8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" to the lovely day. The mother has fallen asleep holding one of the children. And in the second window, the little girl is still blowing bubble gum. She was blowing bubble gum going to this wonderful day. And the bubble was much bigger than it is now. The dog that was going with them was so excited about being out. He's hanging out the window.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset['test'][0]['transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fd288c0-6252-4b92-8f8e-882def541da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1032/1032 [00:01<00:00, 727.83 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 360/360 [00:00<00:00, 982.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "audio_dataset.save_to_disk('/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset_chuncks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f2db5b5-cf19-4de7-88dd-ed10ab28ccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'file_path', 'file_id', 'spk_id', 'prompt_id', 'start_sample', 'end_sample', 'lang', 'transcription', 'tkdname', 'age', 'sex', 'mmse', 'dx', 'audio'],\n",
       "        num_rows: 1032\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'file_path', 'file_id', 'spk_id', 'prompt_id', 'start_sample', 'end_sample', 'lang', 'transcription', 'tkdname_x', 'mmse', 'dx', 'tkdname_y', 'age', 'sex', 'audio'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33593858-e40e-4783-95ad-f2523a1209ca",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1938c2-d848-4308-aa43-b9012e256f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.99s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n",
    "model.tie_weights()\n",
    "#model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93792367-dbe7-4577-a5d9-acfd4b6a72c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262da0aa-5102-4eb4-a76f-ce47f9a1a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang_of_audio(audio, model, processor):\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'}, \n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"audio.wav\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is the language spoken in the audio? choose from English or Mandarine. Give answer as a single word\"},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "    audios = [audio]\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n",
    "    inputs.input_ids = inputs.input_ids.to(model.device)\n",
    "    generate_ids = model.generate(**inputs, max_length=1024)\n",
    "    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e482792-3d3e-411e-b815-fbfe77e494c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang_batch(batch, model, processor):\n",
    "    audio = batch['audio'][\"array\"]\n",
    "    batch['lang'] = detect_lang_of_audio(audio, model, processor)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487856c5-4c99-457f-b916-31de810e9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taukadial_data = taukadial_data.map(detect_lang_batch, fn_kwargs={'model':model, 'processor':processor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74e2c0b8-1a11-4109-8bbc-78f563c839c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 387/387 [00:02<00:00, 166.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 177.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "taukadial_data.save_to_disk(\"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset_lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a6d17c-0bca-4636-b65c-a22bcb4c5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import load_from_disk, Dataset\n",
    "taukadial_data = load_from_disk(\"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3bfe683-8911-4191-8d95-f44039e47e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin\n"
     ]
    }
   ],
   "source": [
    "audio = taukadial_data['train'][3][\"audio\"][\"array\"]\n",
    "print(detect_lang_of_audio(audio, model, processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e530e35-6cbc-4165-8806-138c7b73673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Audio(data=audio, rate=16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854b8c33-293d-4946-a86f-f26a077cf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = Dataset.from_dict(taukadial_data['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98608c-5025-4a1e-a534-30100e19f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = data_part.map(detect_lang_batch, fn_kwargs={'model':model, 'processor':processor})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e4c37-6526-43b0-96df-e0807eef849b",
   "metadata": {},
   "source": [
    "## Zero-shot CI detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0acc5edc-31ae-4850-96d5-99ba6c08cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio\": \"audio.wav\"},\n",
    "        {\"type\": \"text\", \"text\": \"Based on the input audio, indicate only the diagnosis. Choose from: Normal Cognitive Decline or Mild Cognitive Impairment. Give the \\\n",
    "        answer as single word. Use NC for Normal Cognitive Decline and MCI for Mild Cognitive Impairment\"},\n",
    "    ]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66a5a90a-2f80-4485-bda8-3e60b1971fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_CI_of_audio(audio, prompt, model, processor):\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'}, \n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"audio.wav\"},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "    audios = [audio]\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n",
    "    inputs.input_ids = inputs.input_ids.to(model.device)\n",
    "    generate_ids = model.generate(**inputs, max_length=1024)\n",
    "    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0e58ef1-663e-4718-af12-3a25a57fd773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import load_from_disk\n",
    "taukadial_data = load_from_disk(\"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset_lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5710b5da-9d7f-4a47-9c5e-2d0411cbda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Based on the input audio, indicate only the diagnosis. \\\n",
    "Choose from: Normal Cognitive Decline or Mild Cognitive Impairment. \\\n",
    "Give the answer as a single word. Use NC for Normal Cognitive Decline and MCI for Mild Cognitive Impairment\"\n",
    "def detect_CI_batch(batch, prompt, model, processor):\n",
    "    audio = batch['audio'][\"array\"]\n",
    "    batch['dx_detect'] = detect_CI_of_audio(audio, prompt, model, processor)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54197faf-7f19-46a7-aa3a-3d91bca3a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = Dataset.from_dict(taukadial_data['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df6f10bb-9511-451d-8d54-109f7a6b16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.?, ? examples/s]\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/transformers/generation/utils.py:2110: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.2.79s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.2.12s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.97s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.49s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.50s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.54s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.40s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.47s/ examples]\n",
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.1.51s/ examples]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:57<00:00, 11.75s/ examples]\n"
     ]
    }
   ],
   "source": [
    "data_part = data_part.map(detect_CI_batch, fn_kwargs={'prompt': PROMPT,'model':model, 'processor':processor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1e28369-6607-4ad6-a48e-ab226481a94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC', 'NC', 'NC', 'MCI', 'MCI', 'MCI', 'NC', 'NC', 'NC', 'NC']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_part['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bb893-a7cf-41d1-a78d-f6260ab61634",
   "metadata": {},
   "outputs": [],
   "source": [
    "taukadial_data_test_detect = taukadial_data['test'].map(detect_CI_batch, fn_kwargs={'prompt': PROMPT,'model':model, 'processor':processor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b22822-4f86-4cd1-9f16-2363549f7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "taukadial_data_test_detect['dx_detect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc2e960e-16f4-4bd0-93ba-59ead934dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios = [taukadial_data['train'][1][\"audio\"][\"array\"]]\n",
    "inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n",
    "inputs.input_ids = inputs.input_ids.to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51461618-088a-4997-ae01-bd5efe5e8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = model.generate(**inputs, max_length=1024)\n",
    "generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5f63a48-fd3c-4dd1-8485-0b664531ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCI\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de81775d-5280-41cd-a0ff-1baec8b7a04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': None,\n",
       "  'array': array([ 0.00540161,  0.00366211,  0.00131226, ..., -0.00674438,\n",
       "         -0.00759888, -0.00765991]),\n",
       "  'sampling_rate': 16000},\n",
       " 'age': 72,\n",
       " 'sex': 'F',\n",
       " 'mmse': 29,\n",
       " 'dx': 'NC',\n",
       " 'tkdname': 'taukdial-002-2.wav'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taukadial_data['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f9ed1-047c-4493-9f02-85abdf5fde9a",
   "metadata": {},
   "source": [
    "## Compute UAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f2872a33-ec5c-4711-8ed3-6c164f63ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 uar:0.28393282366211303, f1:0.29070179745247254\n",
      "2 uar:0.24359777749926892, f1:0.2464679022293849\n",
      "3 uar:0.3286752725905502, f1:0.3341603515108553\n",
      "4 uar:0.27106571416635333, f1:0.29830489278163697\n",
      "5 uar:0.2607260726072607, f1:0.2863269008741547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, classification_report\n",
    "from datasets import load_from_disk\n",
    "for i in range(5):\n",
    "    result_data_path = f'results/exp19/pred_dataset/data_prompt_{i}/'\n",
    "    results_data = load_from_disk(result_data_path)\n",
    "    y_true = results_data['dx']\n",
    "    y_pred = results_data['dx_pred']\n",
    "    uar = recall_score(y_true, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f'{i+1} uar:{uar}, f1:{f1_macro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "694e8d36-a41d-4879-9462-f570d0bdb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MCI': {'precision': 0.5757575757575758,\n",
       "  'recall': 0.28217821782178215,\n",
       "  'f1-score': 0.3787375415282392,\n",
       "  'support': 202.0},\n",
       " 'NC': {'precision': 0.4619883040935672,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.48024316109422494,\n",
       "  'support': 158.0},\n",
       " 'Unknown': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'accuracy': 0.37777777777777777,\n",
       " 'macro avg': {'precision': 0.34591529328371434,\n",
       "  'recall': 0.2607260726072607,\n",
       "  'f1-score': 0.2863269008741547,\n",
       "  'support': 360.0},\n",
       " 'weighted avg': {'precision': 0.5258255065272609,\n",
       "  'recall': 0.37777777777777777,\n",
       "  'f1-score': 0.423287230115533,\n",
       "  'support': 360.0}}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_true, y_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "060253b0-3df2-4acb-bffe-92cba9cdef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 uar:0.2701197438039543, f1:0.27883167720566093\n",
      "['MCI' 'NC' 'Unknown']\n",
      "2 uar:0.5154553049289892, f1:0.4529469228264409\n",
      "['MCI' 'NC']\n",
      "3 uar:0.5, f1:0.3442622950819672\n",
      "['MCI']\n",
      "4 uar:0.2598162071846282, f1:0.2309248023533738\n",
      "['MCI' 'NC' 'Unknown']\n",
      "5 uar:0.5183792815371763, f1:0.4928003438641736\n",
      "['MCI' 'NC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "for i in range(5):\n",
    "    result_data_path = f'results/exp4/pred_dataset/data_prompt_{i}/'\n",
    "    results_data = load_from_disk(result_data_path)\n",
    "    y_true = results_data['dx']\n",
    "    y_pred = results_data['dx_pred']\n",
    "    uar = recall_score(y_true, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f'{i+1} uar:{uar}, f1:{f1_macro}')\n",
    "    print(np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb078bd7-f9c7-41f6-935c-a10bbbc52cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MCI', 'NC'], dtype='<U3')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f39b79a1-3da3-471f-b3e5-ddf99b381029",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_path = f'results/exp4/pred_dataset/data_prompt_0/'\n",
    "results_data = load_from_disk(result_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a0e7b9ab-4f8a-49af-a7e1-57669d1e0aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'MCI',\n",
       " 'NC',\n",
       " 'NC',\n",
       " 'NC']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11e643-0d4f-47bb-a2a2-ee6f38dac90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8782b-d624-489c-b4a6-e8162f148b40",
   "metadata": {},
   "source": [
    "## results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3418c169-9c7c-4a36-8ff9-a569fbefac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bf21df50-adda-48d6-993b-cad88a2fd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk(\"results/exp1/pred_dataset/data_prompt_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "498f1530-8d64-43ee-a6c1-d40d6d9343a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(100.0)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mask = np.isin(data['response'], ['MCI', 'NC'])\n",
    "np.mean(mask) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3895fa47-f870-427e-8db3-9116c66b1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Transcription': 'She has a tricycle near her and a ladder that’s not tall enough but she has a tricycle near her and she’s trying to catch the cat if the cat finally jumps or falls. There’s a dog who’s attempting to climb up to go vertically up a tree where the dad is sitting on a branch. There are firemen coming and bringing ladders and there’s a firetruck in the background. Thank you so this picture also tells a story.', 'Analysis': 'The speaker has some difficulty describing the image, showing signs of confusion and word-finding difficulties. They struggle to maintain grammatical coherence and have occasional pauses. Vocabulary is limited, with frequent use of simple words.', 'Diagnosis': 'MCI (Mild Cognitive Impairment)'}\n",
      "MCI\n"
     ]
    }
   ],
   "source": [
    "i = 97\n",
    "print(data[i]['response'])\n",
    "print(data[i]['dx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4454ab13-37df-4430-bd7f-130513448610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, there was a family who were going on a trip, mother, father, and three kids. And they are driving along, and the kids, one little boy is hanging out the window with his dog. And the little girl is chewing bubble gum, working on a big, making a big bubble. And the other little boy is waving at people. And then they keep driving until it gets to be dark. And the mother is asleep in the front seat. And the one of the girls is asleep in the back seat.\n",
      "The speaker appears to be describing a scene from a family road trip. The description is vivid and includes specific details about each child’s activity, which helps paint a clear picture in the listener's mind. There are no errors in grammar, vocabulary, or pronunciation; the speech flows smoothly and is easy to understand. The only pause in the speech occurs when the speaker describes the mother falling asleep; otherwise, the speech is continuous and coherent.\n",
      "NC\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = \"\"\"\n",
    "{\"Transcription\": \"Okay, there was a family who were going on a trip, mother, father, and three kids. And they are driving along, and the kids, one little boy is hanging out the window with his dog. And the little girl is chewing bubble gum, working on a big, making a big bubble. And the other little boy is waving at people. And then they keep driving until it gets to be dark. And the mother is asleep in the front seat. And the one of the girls is asleep in the back seat.\", \"Analysis\": \"The speaker appears to be describing a scene from a family road trip. The description is vivid and includes specific details about each child’s activity, which helps paint a clear picture in the listener's mind. There are no errors in grammar, vocabulary, or pronunciation; the speech flows smoothly and is easy to understand. The only pause in the speech occurs when the speaker describes the mother falling asleep; otherwise, the speech is continuous and coherent.\", \"Diagnosis\": \"NC\"}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    data = json.loads(json_string)\n",
    "    #print(data)\n",
    "    print(data['Transcription'])\n",
    "    print(data['Analysis'])\n",
    "    print(data['Diagnosis'])\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b2fe4977-542f-4426-9005-da3c9acf95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_per_one_word(exp_path):\n",
    "    output = []\n",
    "    for prompts_id in range(5):\n",
    "        data = load_from_disk(os.path.join(exp_path,'pred_dataset',f'data_prompt_{prompts_id}'))\n",
    "        mask = np.isin(data['response'], ['MCI', 'NC'])\n",
    "        output.append((prompts_id,np.mean(mask) * 100))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a9a93906-0383-4304-bcc3-46577c43178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute UAC and MF1 per prompt, return avg, std, max, mvote, index of max P\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(exp_path, lang=None):\n",
    "    UAR_scores = []\n",
    "    f1_scores = []\n",
    "    max_prompt = ''\n",
    "    best_UAR = 0\n",
    "    f1_of_best_UAR = 0\n",
    "    for prompts_id in range(5):\n",
    "        data = load_from_disk(os.path.join(exp_path,'pred_dataset',f'data_prompt_{prompts_id}'))\n",
    "        if lang:\n",
    "            data = data.filter(lambda x: x['lang']==lang)\n",
    "        y_ref = data['dx']\n",
    "        y_pred = data['dx_pred']\n",
    "        uar = recall_score(y_true=y_ref, y_pred=y_pred, average='macro')\n",
    "        f1 = f1_score(y_true=y_ref, y_pred=y_pred, average='macro')\n",
    "        UAR_scores.append(uar)\n",
    "        f1_scores.append(f1)\n",
    "        if uar > best_UAR:\n",
    "            max_prompt = prompts_id\n",
    "            best_UAR = uar\n",
    "            f1_of_best_UAR = f1\n",
    "    return(np.average(UAR_scores), np.std(UAR_scores), np.average(f1_scores), np.std(f1_scores), max_prompt, best_UAR, f1_of_best_UAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e55f7f-1f95-4f19-b963-eae886c41220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for exp in os.listdir('results'):\n",
    "    exp_path = os.path.join('results',exp)\n",
    "    output = get_per_one_word(exp_path)\n",
    "    for item in output:\n",
    "        print(exp, item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "9c5e9f1a-d83b-49c5-afe7-cd5294d7e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(0.5183507549361208), np.float64(0.013920818770241691), np.float64(0.4777249684743978), np.float64(0.04442557437901736), 3, 0.537940379403794, 0.5206043549103836)\n",
      "(np.float64(0.563131313131313), np.float64(0.05462043566276347), np.float64(0.47401821734647775), np.float64(0.11495056995365743), 4, 0.6363636363636364, 0.6362396806901851)\n",
      "(np.float64(0.532989352989353), np.float64(0.031071660604493307), np.float64(0.4768675280124549), np.float64(0.07639073987488168), 4, 0.5824733824733825, 0.5733585445215776)\n"
     ]
    }
   ],
   "source": [
    "Exp_path = 'results/exp10/'\n",
    "print(compute_metrics(Exp_path, lang='English'))\n",
    "print(compute_metrics(Exp_path, lang='Mandarin'))\n",
    "print(compute_metrics(Exp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a9556ceb-a4d0-422c-a087-c3597b52eef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(np.asarray(['MCI','NC']) == 'MCI' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3c57de4c-34a4-4c60-b106-6c160b1c1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all_prompts = []\n",
    "for prompts_id in range(5):\n",
    "    data = load_from_disk(os.path.join(Exp_path,'pred_dataset',f'data_prompt_{prompts_id}'))\n",
    "    y_ref = data['dx']\n",
    "    y_pred = data['dx_pred']\n",
    "    y_pred_all_prompts.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "8ae77ecc-8d6a-4bc4-9063-ed7bc9a8b558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MCI', 'MCI', 'MCI', 'MCI', 'MCI'], dtype='<U3')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(y_pred_all_prompts)[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6f650-764d-45b5-9475-59a99a95f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c35fcc21-37ad-4d93-a3ab-22d0b9255ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['age', 'sex', 'mmse', 'dx', 'tkdname', 'lang', 'response', 'dx_pred'],\n",
       "    num_rows: 60\n",
       "})"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c53ee-efb1-4dfc-ba65-0da11cd21360",
   "metadata": {},
   "source": [
    "## Zero-shot v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13d19036-a30a-4637-9996-9f40280a1621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.97s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6928deaa-ec14-4108-9fad-45c9badaeaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_audio(audio, prompt, model, processor):\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'}, \n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"audio.wav\"},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "    audios = [audio]\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True, sampling_rate=16_000).to('cuda')\n",
    "    #inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    generate_ids = model.generate(**inputs, max_length=1024)\n",
    "    generate_ids = generate_ids[:, inputs.input_ids.size(1):] # to get only the generated text\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f4db39a2-f96e-4cae-9d17-8b37c7d15c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import load_from_disk\n",
    "taukadial_data = load_from_disk(\"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset_lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "70308d2a-5c37-4815-b2ad-15acc54db46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_CI_batch(batch, prompt, model, processor):\n",
    "    audio = batch['audio'][\"array\"]\n",
    "    response = prompt_with_audio(audio, prompt, model, processor)\n",
    "    batch['response'] = response\n",
    "    if \"MCI\" in response:\n",
    "        batch['dx_pred'] = \"MCI\"\n",
    "    elif \"NC\" in response:\n",
    "        batch['dx_pred'] = \"NC\"\n",
    "    else:\n",
    "        batch['dx_pred'] = \"Unknown\"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe82b5fe-c18e-43a9-bc41-d877d2da1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_results(data, ref_col, pred_col):\n",
    "    return classification_report(data[ref_col], data[pred_col], output_dict=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fea0e00-d144-4986-a804-54ff95f52d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_CI = \"Based on the input audio, indicate only the diagnosis. \\\n",
    "Choose from: Normal Cognitive Decline or Mild Cognitive Impairment. \\\n",
    "Give the answer as a single word. Use NC for Normal Cognitive Decline and MCI for Mild Cognitive Impairment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f4a09f24-9a93-4c7d-a3d0-f3b137ac7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(prompts_file):\n",
    "    with open(prompts_file) as f:\n",
    "        prompts = f.read().splitlines()\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "43ba6109-b54b-40f6-8571-e939b2591a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts(\"prompts/prompts_set2_zero_shot_CoT.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "08eab143-c372-40a3-8be0-d7e6fd41fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = prompts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b090d6a1-7b83-432d-a7e0-6038439bab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First, transcribe the speech from the input audio. Then, analyze both the audio and the transcription to determine the diagnosis. Choose from: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Provide your response as a single word: NC or MCI.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5c555ec6-f70d-4417-b5a6-de185d75b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data_part = Dataset.from_dict(taukadial_data['test'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b04cd96c-6808-420b-9717-391e5dfb5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 14.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_part.save_to_disk('/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset_10samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "224ebd7d-75a6-45d7-8fd8-08cfc7f56331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [43:25<00:00, 260.59s/ examples]\n"
     ]
    }
   ],
   "source": [
    "data_part = data_part.map(detect_CI_batch, fn_kwargs={'prompt': prompts[0],'model':model, 'processor':processor});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "85ed0971-be63-4619-8646-f878d9a1e857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MCI',\n",
       " 'MCI',\n",
       " \"The speech content is '这个小孩子就玩电吧，把这个大人把把叫。那个狗汪汪叫，把这个猫咪听他讲讲，舌头伸出来。那这个是什么意思？是那个那个花瓶水倒下来，大概这样吧。'\",\n",
       " \"The speech in the audio is: 'Okay, there was a family who were going on a trip mother, father, and three kids and they're driving along and the kids one little boy is hanging out the window with his dog and the little girl is chewing bubble gum working on a big making a big bubble and the other little boy is waving at people and then they keep driving until it gets to be dark and the mother is asleep in the front seat and the one of the girls is asleep in the back.' Based on this description, it seems that the family is just going on a routine trip, without any signs of cognitive decline or impairment. Therefore, the diagnosis would be NC (Normal Cognitive Decline).\",\n",
       " \"The speech in the audio is transcribed as follows: 'the little girl is riding her tricycle and her dad is walking along with her and the cat and the cat comes out and runs up the tree because the dog is chasing the cat. so the father gets the ladder and puts it up and climbs up the tree. when he gets up to the top, he gets stuck and they have to call the fire department. and two firemen come with another ladder to rescue the father.' Based on this speech content, the diagnosis is not straightforward and requires additional context. The speech does not provide explicit information regarding cognitive function, memory, or language abilities. Therefore, without further context, it's not possible to accurately diagnose the individual as having either NC (normal cognitive decline) or MCI (mild cognitive impairment).\",\n",
       " 'MCI',\n",
       " \"The speech in the audio is transcribed as: 'Okay. Ah the top picture is the beginning of a trip with the family um it looks like a lot of people are part of this family and going on this trip um they look like they have there might be going to the beach because I.' \\n\\nBased on the content provided in the transcription, it appears that the speaker is describing the beginning of a family trip, which suggests that they are likely going to the beach. Therefore, the diagnosis for this individual would be MCI (Mild Cognitive Impairment), as the speech indicates some decline in cognitive function but does not yet meet the criteria for full-blown dementia.\",\n",
       " \"The speech in the audio is 'um the cat unfortunately when up a tree does not want to come down even though the girls trying to call to it and reach it so.' This sentence does not provide enough context to determine a specific cognitive decline or impairment diagnosis. Therefore, the answer is 'NC' for normal cognitive decline.\",\n",
       " 'NC',\n",
       " 'MCI']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_part['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b1e9886-2f02-4aac-aa5a-c9c1b77867d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "d = classification_report(data_part['dx'], data_part['dx_detect'], output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e94ca6d-a002-40ea-8211-774148efd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the input audio and determine the diagnosis. Select from the following options: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Provide your response as a single word: NC or MCI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [23:03<00:00, 11.53s/ examples]\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen to the provided audio and classify the cognitive condition.            Choose either NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment.            Respond with only one word: NC or MCI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [23:10<00:00, 11.58s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the speech patterns in the input audio, identify the cognitive status.            Use NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment.            Your response should be a single word: NC or MCI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [23:08<00:00, 11.57s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the given speech sample, determine the cognitive condition.            Select one of the following labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment.            Reply with only one word: NC or MCI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [21:51<00:00, 10.93s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assess the cognitive condition based on the input audio.            Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment.            Output only NC or MCI as your response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [30:05<00:00, 15.05s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the input audio, indicate only the diagnosis.            Choose from: Normal Cognitive Decline or Mild Cognitive Impairment.            Give the answer as a single word. Use NC for Normal Cognitive Decline and MCI for Mild Cognitive Impairment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [32:46<00:00, 16.39s/ examples]\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "results = []\n",
    "pred_dataset = []\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    data_pred = taukadial_data['test'].map(detect_CI_batch, fn_kwargs={'prompt': prompt,'model':model, 'processor':processor});\n",
    "    pred_dataset.append(deepcopy(data_pred))\n",
    "    results_d = get_class_results(data_pred, 'dx','dx_pred')\n",
    "    results.append({\"prompt\":prompt, \n",
    "                    \"MCI_recall\":results_d[\"MCI\"][\"recall\"],\n",
    "                   \"MCI_precision\":results_d[\"MCI\"][\"precision\"],\n",
    "                   \"MCI_f1-score\":results_d[\"MCI\"][\"f1-score\"],\n",
    "                   \"MCI_support\":results_d[\"MCI\"][\"support\"],\n",
    "                   \"NC_recall\":results_d[\"NC\"][\"recall\"],\n",
    "                   \"NC_precision\":results_d[\"NC\"][\"precision\"],\n",
    "                   \"NC_f1-score\":results_d[\"NC\"][\"f1-score\"],\n",
    "                   \"NC_support\":results_d[\"NC\"][\"support\"],\n",
    "                   \"f1_score_macro\":results_d['macro avg']['f1-score'],\n",
    "                   \"f1_score_weighted\":results_d['weighted avg']['f1-score']})\n",
    "df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64d31e5a-ca8f-4ec5-9de3-c597f8bfda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c215bf6-7826-4b5a-9d42-a8f3a4b8c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MCI': {'precision': 0.525,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.6885245901639344,\n",
       "  'support': 63.0},\n",
       " 'NC': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 57.0},\n",
       " 'accuracy': 0.525,\n",
       " 'macro avg': {'precision': 0.2625,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.3442622950819672,\n",
       "  'support': 120.0},\n",
       " 'weighted avg': {'precision': 0.275625,\n",
       "  'recall': 0.525,\n",
       "  'f1-score': 0.3614754098360656,\n",
       "  'support': 120.0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e74efc62-ae5a-43b9-b6c5-510141711139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3442622950819672"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_d['weighted avg']['f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19c26809-0b77-4953-a3e6-641b820302c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tmp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9869dd2-5b66-452f-b74b-29e6a85a7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "x = deepcopy(data_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e00afec-d687-4127-a596-c7a50bd75746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 836.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_part.remove_columns(['audio']).save_to_disk('data_part_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813dda34-83d4-4d7f-ba03-82af72c6aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['age', 'sex', 'mmse', 'dx', 'tkdname', 'lang'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_part_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a30e66-85e9-44a5-b25a-f9f7cb49663a",
   "metadata": {},
   "source": [
    "# Zero-shot with customized info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8e778-2323-4607-afab-7b8a71cef543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af94425-4820-48d3-86fb-880b37ffcf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.92s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c5dd41-f69d-45d5-a9c8-dbd1359ffbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_audio(audio, prompt, model, processor):\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'}, \n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"audio.wav\"},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "    audios = [audio]\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True, sampling_rate=16_000).to('cuda')\n",
    "    #inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    generate_ids = model.generate(**inputs, max_length=1024)\n",
    "    generate_ids = generate_ids[:, inputs.input_ids.size(1):] # to get only the generated text\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3372e2b6-a888-40a4-82cf-b42a7a84efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import load_from_disk\n",
    "taukadial_data = load_from_disk(\"/media/unsw/17D1-7AB7/mostafa/datasets/TAUKADIAL_dataset_10samples/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfa82564-5a32-46d9-81e0-08350d32f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'age', 'sex', 'mmse', 'dx', 'tkdname', 'lang'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taukadial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c992941-58f7-40a7-9199-c8afbd3a7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map = {'M':'male','F':'female'}\n",
    "def detect_CI_with_filling_batch(batch, prompt, model, processor):\n",
    "    audio = batch['audio'][\"array\"]\n",
    "    if '[AGE]' in prompt:\n",
    "        prompt = prompt.replace('[AGE]',str(batch['age']))\n",
    "    if '[LANGUAGE]' in prompt:\n",
    "        prompt = prompt.replace('[LANGUAGE]',batch['lang'])\n",
    "    if '[GENDER]' in prompt:\n",
    "        prompt = prompt.replace('[GENDER]', gender_map[batch['sex']])\n",
    "    batch['prompt'] = prompt\n",
    "    response = prompt_with_audio(audio, prompt, model, processor)\n",
    "    batch['response'] = response\n",
    "    if \"MCI\" in response:\n",
    "        batch['dx_pred'] = \"MCI\"\n",
    "    elif \"NC\" in response:\n",
    "        batch['dx_pred'] = \"NC\"\n",
    "    else:\n",
    "        batch['dx_pred'] = \"Unknown\"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38bb27e5-edc9-4e99-a54d-ad059e02b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_results(data, ref_col, pred_col):\n",
    "    return classification_report(data[ref_col], data[pred_col], output_dict=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4706a450-19da-491b-b5dd-fbece3fe65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_CI = \"Based on the input audio, indicate only the diagnosis. \\\n",
    "Choose from: Normal Cognitive Decline or Mild Cognitive Impairment. \\\n",
    "Give the answer as a single word. Use NC for Normal Cognitive Decline and MCI for Mild Cognitive Impairment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee737e39-4d31-47da-a07a-9021f05ac25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(prompts_file):\n",
    "    with open(prompts_file) as f:\n",
    "        prompts = f.read().splitlines()\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb6f1c0c-d2ac-48b6-a4e1-99c6df2bf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts(\"tmp_prompt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0416fe06-982e-40bd-8ebf-26effdc56ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analyze the input audio, in which a [AGE]-year-old [GENDER] describes a picture in [LANGUAGE] as prompted by a clinician. Determine the diagnosis based on speech patterns. Select from: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Provide your response as a single word: NC or MCI.',\n",
       " 'Listen to the provided audio, in which a [AGE]-year-old [GENDER] describes a picture in [LANGUAGE] as prompted by a clinician. Classify the cognitive condition. Choose either NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Respond with only one word: NC or MCI.',\n",
       " 'Based on the speech patterns in the input audio, in which a [AGE]-year-old [GENDER] describes a picture in [LANGUAGE], identify the cognitive status. Use NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Your response should be a single word: NC or MCI.',\n",
       " 'From the given speech sample, in which a [AGE]-year-old [GENDER] describes a picture in [LANGUAGE] as prompted by a clinician, determine the cognitive condition. Select one of the following labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Reply with only one word: NC or MCI.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a [AGE]-year-old [GENDER] describes a picture in [LANGUAGE] as prompted by a clinician. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cdc5c82-cb46-4a5c-9cc0-3f7d77e67a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.                         | 0/10 [00:00<?, ? examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:49<00:00, 10.98s/ examples]\n"
     ]
    }
   ],
   "source": [
    "data_part = taukadial_data.map(detect_CI_with_filling_batch, fn_kwargs={'prompt': prompts[0],'model':model, 'processor':processor});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3302d107-4b92-4d9a-92dc-a7223f301136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC', 'NC', 'NC', 'MCI', 'MCI', 'MCI', 'NC', 'NC', 'NC', 'NC']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_part['dx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3cc48-0701-43e9-ade7-148cf48fdf96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "971c27a3-1212-48f7-9d0d-a7d7bed3d33d",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b398d568-7223-4d8b-9a46-37395fbf26d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/unsw/17D1-7AB7/mostafa/audioLLM/Qwen'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2614eddc-aab8-40b4-9424-0d42006ce33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_length = processor.feature_extractor.n_samples\n",
    "def process_audio( audio_url: str) -> np.ndarray:\n",
    "    try:\n",
    "        audio, sr = librosa.load(BytesIO(urlopen(audio_url).read()), sr=processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        if len(audio) > max_length:\n",
    "            if debug:\n",
    "                print(f\"Trimming audio to {max_length} samples\")\n",
    "            audio = audio[:max_length]\n",
    "        return audio.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing audio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ccd19d57-56e0-4d1c-b873-14e91ec14168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'name', 'url', 'license', '__index_level_0__'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85bd9eb6-205d-4e01-986a-c1db9b87e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples = []\n",
    "audios = []\n",
    "combined_texts = []\n",
    "task_prompt = \"Identify the bird species in the audio clip\"\n",
    "for example in eval_dataset:\n",
    "    audio = process_audio(example[\"url\"])\n",
    "    audios.append(audio)\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\":[\n",
    "            {\"type\": \"audio\", \"audio_url\": example[\"url\"]},\n",
    "            {\"type\": \"text\", \"text\": task_prompt}\n",
    "        ]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"name\"]}\n",
    "    ]\n",
    "    combined_text = processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=False, tokenize=False  #Here add_generation_prompt=False as it's training and the answer is in the conversation, and tokenize=false as we will do tokenize later\n",
    "    )\n",
    "\n",
    "    combined_texts.append(combined_text)\n",
    "    valid_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0957281-26e4-40be-bc0c-8e2f5de7353b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ad4ae87-2418-4eff-87e2-599601ec3225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(\n",
    "                text=combined_texts,\n",
    "                audios=audios,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1cb8eb0e-657f-4639-94f5-192368fcd120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|AUDIO|>']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.convert_ids_to_tokens([151646])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae9b05e3-4a82-4e88-861c-bf1904f0d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_start_tokens = processor.tokenizer.encode(\"<|im_start|>assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e000d03-00e6-4ed4-8156-1998fa44103e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant_start_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65bf0426-8170-402a-b5aa-4d89d2bdaff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151643, 151644,   8948,  ...,  40652, 151645,    198],\n",
       "        [151643, 151644,   8948,  ...,   7183, 151645,    198],\n",
       "        [151643, 151643, 151643,  ...,  77462, 151645,    198],\n",
       "        ...,\n",
       "        [151643, 151643, 151643,  ...,   2468, 151645,    198],\n",
       "        [151644,   8948,    198,  ...,  11346, 151645,    198],\n",
       "        [151644,   8948,    198,  ...,    838, 151645,    198]]), 'attention_mask': tensor([[0, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'input_features': tensor([[[-1.3869, -1.3869, -1.3869,  ..., -1.3869, -1.3869, -1.3869],\n",
       "         [-1.3869, -1.3869, -1.3869,  ..., -1.3869, -1.3869, -1.3869],\n",
       "         [-1.3869, -1.3869, -1.3869,  ..., -1.3869, -1.3869, -1.3861],\n",
       "         ...,\n",
       "         [-1.3869, -1.3869, -1.3869,  ..., -1.3100, -1.3505, -1.2018],\n",
       "         [-1.3869, -1.3869, -1.3869,  ..., -1.3869, -1.3869, -1.3602],\n",
       "         [-1.3869, -1.3869, -1.3869,  ..., -1.3869, -1.3869, -1.3869]],\n",
       "\n",
       "        [[-0.0027,  0.1805,  0.1724,  ...,  0.0148,  0.3042,  0.3864],\n",
       "         [ 0.0949,  0.2781,  0.2700,  ...,  0.1123,  0.4018,  0.4840],\n",
       "         [ 0.2526,  0.2454,  0.2975,  ...,  0.2171,  0.3988,  0.5167],\n",
       "         ...,\n",
       "         [-0.3472, -0.3766, -0.2810,  ..., -0.4127, -0.3866, -0.3759],\n",
       "         [-0.4877, -0.4974, -0.3826,  ..., -0.5879, -0.4621, -0.5020],\n",
       "         [-0.6707, -0.6707, -0.6707,  ..., -0.6707, -0.6707, -0.6707]],\n",
       "\n",
       "        [[-0.9418, -0.9418, -0.9418,  ..., -0.9418, -0.9418, -0.9418],\n",
       "         [-0.9418, -0.9418, -0.9418,  ..., -0.9418, -0.9418, -0.9418],\n",
       "         [-0.9418, -0.9418, -0.9418,  ..., -0.9418, -0.9418, -0.9418],\n",
       "         ...,\n",
       "         [-0.1052, -0.0503, -0.0037,  ..., -0.9418, -0.9418, -0.9418],\n",
       "         [-0.3540, -0.1579, -0.1736,  ..., -0.9418, -0.9418, -0.9418],\n",
       "         [-0.4021, -0.4848, -0.5563,  ..., -0.9418, -0.9418, -0.9418]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1275,  0.0082, -0.0703,  ..., -1.5000, -1.5000, -1.5000],\n",
       "         [-0.0299,  0.1057,  0.0273,  ..., -1.5000, -1.5000, -1.5000],\n",
       "         [ 0.1814,  0.2198,  0.2267,  ..., -1.5000, -1.5000, -1.5000],\n",
       "         ...,\n",
       "         [-0.9474, -0.9199, -0.9525,  ..., -1.5000, -1.5000, -1.5000],\n",
       "         [-0.8614, -1.0415, -1.0384,  ..., -1.5000, -1.5000, -1.5000],\n",
       "         [-1.1609, -1.3330, -1.2412,  ..., -1.5000, -1.5000, -1.5000]],\n",
       "\n",
       "        [[-0.5606, -0.4701, -0.5448,  ..., -0.8639, -0.5259, -0.6433],\n",
       "         [-0.4630, -0.3726, -0.4472,  ..., -0.7663, -0.4284, -0.5457],\n",
       "         [-0.3396, -0.2894, -0.3603,  ..., -0.3234, -0.2203, -0.5088],\n",
       "         ...,\n",
       "         [-0.5400, -0.4444, -0.5953,  ..., -0.4338, -0.5537, -0.5538],\n",
       "         [-0.5895, -0.4400, -0.6070,  ..., -0.4695, -0.3054, -0.4797],\n",
       "         [-0.8205, -0.8839, -0.8859,  ..., -0.7561, -0.4429, -0.5072]],\n",
       "\n",
       "        [[-0.7539, -0.7539, -0.7539,  ..., -0.7539, -0.7539, -0.7539],\n",
       "         [-0.7539, -0.7539, -0.7539,  ..., -0.7539, -0.7539, -0.7539],\n",
       "         [-0.7539, -0.6743, -0.7539,  ..., -0.7539, -0.7539, -0.7539],\n",
       "         ...,\n",
       "         [-0.0974, -0.0402, -0.1232,  ..., -0.0226,  0.0454,  0.0369],\n",
       "         [-0.3058, -0.1382, -0.1664,  ..., -0.2259, -0.0664, -0.1665],\n",
       "         [-0.5242, -0.5883, -0.4070,  ..., -0.5921, -0.4376, -0.4459]]]), 'feature_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c39cd8-a205-4a69-9780-507f413972c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load audio\n",
    "TASK_PROMPT = \"What type of bird do you hear? output only the type of the bird\"\n",
    "audio_url = \"https://xeno-canto.org/841748/download\"\n",
    "audio_data, sample_rate = librosa.load(BytesIO(urlopen(audio_url).read()), sr=None)\n",
    "#play audio\n",
    "display(Audio(audio_data, rate=sample_rate))\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": audio_url},\n",
    "        {\"type\": \"text\", \"text\": TASK_PROMPT},\n",
    "    ]}\n",
    "]\n",
    "# process conversation\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt= True, tokenize=False)\n",
    "if debug:\n",
    "    print(f\"Templated text:\\n{text}\")\n",
    "    \n",
    "audios = []\n",
    "\n",
    "for message in conversation:\n",
    "    if isinstance(message[\"content\"], list):\n",
    "        for item in message[\"content\"]:\n",
    "            if isinstance(item, dict) and item[\"type\"] == \"audio\" and \"audio_url\" in item:\n",
    "                try:\n",
    "                    sr = int(processor.feature_extractor.sampling_rate)\n",
    "                    aduio_data = librosa.load(BytesIO(urlopen(item[\"audio_url\"]).read()), sr=sr)[0]\n",
    "                    audios.append(aduio_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"failed to process audio from {item['audio_url']}, {e}\")\n",
    "inputs = processor(\n",
    "    text=text,\n",
    "    audios=audios,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_length=512)\n",
    "\n",
    "print(f\"Input shape: {inputs.input_ids.size(1)}, Generated shape: {generate_ids.shape}\")\n",
    "generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2fe8799-ba77-475f-bae4-cd688fefa1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bird is a sparrow.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "2408b5e2-bafd-4477-8db8-f44498c86bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90755b6e-9332-457a-95df-3b3b249905d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unsw/anaconda3/envs/qa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5072c9eb-c79e-44e4-b0fe-dacce1b61cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk('results/tmp/pred_dataset/data_prompt_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea4c844-9949-4af6-a0ac-da4ed884bed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC', 'MCI', 'NC', 'NC', 'NC', 'NC', 'NC', 'NC', 'NC', 'NC']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['dx_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72eaa19b-46a4-49ad-ac88-1b8ae724e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC', 'NC', 'NC', 'MCI', 'MCI', 'MCI', 'NC', 'NC', 'NC', 'MCI']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34915d50-a286-45e0-a1ce-76cf36b0e6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assess the cognitive condition based on the input audio, in which a 70-year-old female describes the Cookie Theft picture in Mandarin as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 70-year-old female describes the Cookie Theft picture in Mandarin as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 70-year-old female describes the Cookie Theft picture in Mandarin as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 91-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 91-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 91-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 71-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 71-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 71-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.',\n",
       " 'Assess the cognitive condition based on the input audio, in which a 88-year-old female describes the Cookie Theft picture in English as part of a clinician-guided task. Analyze both linguistic and acoustic features, including pauses, hesitations, and richness of vocabulary. The dataset includes recordings in both Chinese and English. Indicate the diagnosis using one of these labels: NC for Normal Cognitive Decline or MCI for Mild Cognitive Impairment. Output only NC or MCI as your response.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb75d6e-0107-403d-9806-6045dd604fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
